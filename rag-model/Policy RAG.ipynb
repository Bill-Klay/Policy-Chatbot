{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import tiktoken\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from docx import Document\n",
    "from chromadb.config import Settings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microservices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic file reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_paragraph_delimiter(text_sample, possible_delimiters=[\"\\n\\n\", \"\\n \\n\", \"\\n\\t\\n\"]):\n",
    "    \"\"\"\n",
    "    Detects the most common paragraph delimiter in a text sample.\n",
    "\n",
    "    Parameters:\n",
    "    - text_sample (str): A sample of the text to analyze.\n",
    "    - possible_delimiters (list of str): A list of possible paragraph delimiters.\n",
    "\n",
    "    Returns:\n",
    "    - The most common paragraph delimiter found in the sample.\n",
    "    \"\"\"\n",
    "    delimiter_counts = {delimiter: text_sample.count(delimiter) for delimiter in possible_delimiters}\n",
    "    # Sort delimiters by their occurrence count in descending order\n",
    "    sorted_delimiters = sorted(delimiter_counts, key=delimiter_counts.get, reverse=True)\n",
    "    # Return the most common delimiter\n",
    "    delimiter = sorted_delimiters[0] if sorted_delimiters else None\n",
    "#     print(delimeter)\n",
    "    return delimeter\n",
    "\n",
    "def split_paragraph_into_overlapping_chunks(paragraph, token_limit=28000, overlap_size=1000):\n",
    "    \"\"\"\n",
    "    Splits a paragraph into smaller chunks based on the token_limit, with an\n",
    "    overlap of overlap_size characters between consecutive chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - paragraph (str): The paragraph to be split.\n",
    "    - token_limit (int): The maximum number of characters per chunk.\n",
    "    - overlap_size (int): The number of characters to overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of overlapping text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index < len(paragraph):\n",
    "        # If we're not at the start, move back to create overlap\n",
    "        if start_index > 0:\n",
    "            start_index = max(start_index - overlap_size, 0)\n",
    "\n",
    "        end_index = start_index + token_limit\n",
    "        chunk = paragraph[start_index:end_index]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Break if we're at the end of the paragraph\n",
    "        if end_index >= len(paragraph):\n",
    "            break\n",
    "\n",
    "        start_index = end_index\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def split_text_into_paragraphs_and_chunks(text, token_limit=28000, overlap_size=1000):\n",
    "    \"\"\"\n",
    "    Splits the given text into paragraphs and then into overlapping chunks with\n",
    "    a maximum of token_limit characters. If a paragraph is larger than token_limit,\n",
    "    it's further split into smaller chunks with overlap for better context.\n",
    "    \"\"\"\n",
    "    delimiter = detect_paragraph_delimiter(text[:1000])  # Sample the first 1000 characters\n",
    "#     print(f\"{detect_paragraph_delimiter(text[:1000])}\")\n",
    "    if not delimiter:\n",
    "        delimiter = \"\\n\\n\"  # Default to \"\\n\\n\" if no delimiter is detected\n",
    "    paragraphs = text.split(delimiter)\n",
    "#     print(paragraphs)\n",
    "    all_chunks = []\n",
    "#     print(len(paragraphs))\n",
    "#     print(type(paragraphs))\n",
    "    for paragraph in paragraphs:\n",
    "        if len(paragraph) > token_limit:\n",
    "            # Split large paragraphs into smaller overlapping chunks\n",
    "            chunks = split_paragraph_into_overlapping_chunks(paragraph, token_limit, overlap_size)\n",
    "            all_chunks.extend(chunks)\n",
    "        else:\n",
    "            all_chunks.append(paragraph)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a the file path\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "    - full_text (str): File content.\n",
    "    \"\"\"\n",
    "    full_text = \"\"\n",
    "    if file_path.endswith('.pdf'):\n",
    "        with fitz.open(file_path) as doc:\n",
    "            for page in doc:\n",
    "                full_text += page.get_text()\n",
    "#                 print(detect_paragraph_delimiter(full_text))\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = Document(file_path)\n",
    "        full_text = ' '.join(paragraph.text for paragraph in doc.paragraphs)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r') as file:\n",
    "            full_text = file.read()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "def split_into_chunks(text, token_limit=28000):\n",
    "    \"\"\"\n",
    "    Splits the given text into chunks with a maximum of token_limit tokens.\n",
    "    Token limit is an estimate of character count hence its more than the required 8191 for text-embedding-3-large model.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to be split.\n",
    "    - token_limit (int): The maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(' '.join(current_chunk + [word])) > token_limit:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name=\"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "    \"\"\"\n",
    "    Takes in the chunk of text from the file content and return the vector embeddings using the specified model\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The chunk of text from the file read\n",
    "    - model (str): The model to use for embedding. As of writing this, text-embedding-3-large is the latest\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of the vector embeddings for the text provided.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "def populate_embeddings_to_chromadb(folder_path):\n",
    "    # Read each file in the folder, break it into chunks and return a list of chuncks for vectorization\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Read the file content\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_content = read_text_from_file(file_path)\n",
    "#             paragraphs = file_content.split('\\n \\n')\n",
    "#             text_chunks = split_into_chunks(file_content)\n",
    "            text_chunks = split_text_into_paragraphs_and_chunks(file_content)\n",
    "#     return text_chunks\n",
    "        # Tokenize and chunk the file content\n",
    "        vector = []\n",
    "        print(filename)\n",
    "        for index, chunk in enumerate(text_chunks, start=1):\n",
    "            try:\n",
    "#                 vector.append(get_embedding(chunk))\n",
    "                unique_id = f\"{filename}_{index}\"\n",
    "                vector = get_embedding(chunk)\n",
    "                store_vector_in_chromadb(vector, filename, chunk, unique_id)\n",
    "            except Exception as e:\n",
    "                print(\"Could not embed the text chunk for file (check token limit): \", filename)\n",
    "                print(e)\n",
    "            print(num_tokens_from_string(chunk))\n",
    "\n",
    "#         if len(vector) > 1:\n",
    "            # Compute the mean embedding\n",
    "#             vector = np.mean(vector, axis=0)\n",
    "\n",
    "            # Optionally, normalize the mean embedding\n",
    "            # vector = np.array(mean_embedding / np.linalg.norm(mean_embedding))\n",
    "#             vector = vector.tolist()\n",
    "\n",
    "#         print(type(vector))\n",
    "        print(len(text_chunks))\n",
    "        print(type(text_chunks))\n",
    "#         store_vector_in_chromadb(vector, filename, ' '.join(text_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save vector to Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_vector_in_chromadb(vector, filename, text, unique_id):\n",
    "    # Placeholder for storing the vector in ChromaDB\n",
    "    # Implement according to your ChromaDB setup\n",
    "    \n",
    "    client = chromadb.PersistentClient(path=\".chromadb/\",settings=Settings(allow_reset=True))\n",
    "    collection = client.get_or_create_collection(name=\"policy_files\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "        \n",
    "    collection.add(documents = [text], embeddings = vector, metadatas = [{\"source\": filename}], ids = [unique_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vector_in_chromadb(query):\n",
    "    client = chromadb.PersistentClient(path=\".chromadb/\",settings=Settings(allow_reset=True))\n",
    "    collection = client.get_collection(name=\"policy_files\")\n",
    "    vector = get_embedding(query)\n",
    "\n",
    "#     openai_embeddings = OpenAIEmbeddings(openai_api_key = os.environ['OPENAI_API_KEY'], model = \"text-embedding-3-large\")\n",
    "#     langchain_chroma = Chroma(\n",
    "#         client = client,\n",
    "#         collection_name=\"policy_files\",\n",
    "#         embedding_function=openai_embeddings,\n",
    "#     )\n",
    "#     docs = langchain_chroma.similarity_search(query)\n",
    "#     return docs[0].page_content\n",
    "\n",
    "    return collection.query(query_embeddings = vector, n_results=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT-4 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "GPT_CLIENT = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if _name == \"main\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions-with-HCPs-Policy-Field.pdf\n",
      "337\n",
      "695\n",
      "181\n",
      "130\n",
      "278\n",
      "125\n",
      "181\n",
      "310\n",
      "192\n",
      "145\n",
      "118\n",
      "7\n",
      "235\n",
      "152\n",
      "308\n",
      "575\n",
      "652\n",
      "366\n",
      "1106\n",
      "289\n",
      "1516\n",
      "208\n",
      "931\n",
      "13\n",
      "328\n",
      "675\n",
      "30\n",
      "34\n",
      "155\n",
      "276\n",
      "54\n",
      "204\n",
      "35\n",
      "62\n",
      "220\n",
      "77\n",
      "68\n",
      "1109\n",
      "38\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMy Projects\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPolicy Chatbot\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mrag-model\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m text_chunks \u001b[38;5;241m=\u001b[39m populate_embeddings_to_chromadb(folder_path)\n\u001b[1;32m----> 5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat would you like to know?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(read_vector_in_chromadb(query))\n",
      "File \u001b[1;32mD:\\My Projects\\Policy Chatbot\\rag-model\\venv_rag\\lib\\site-packages\\ipykernel\\kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\My Projects\\Policy Chatbot\\rag-model\\venv_rag\\lib\\site-packages\\ipykernel\\kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Data files folder path\n",
    "folder_path = \"D:\\\\My Projects\\\\Policy Chatbot\\\\rag-model\\\\files\"\n",
    "\n",
    "text_chunks = populate_embeddings_to_chromadb(folder_path)\n",
    "query = input(\"What would you like to know?\")\n",
    "print(read_vector_in_chromadb(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path=\".chromadb/\", settings=Settings(allow_reset=True))\n",
    "client.heartbeat() # returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n",
    "client.reset() # Empties and completely resets the database. ⚠️ This is destructive and not reversible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "venv_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
